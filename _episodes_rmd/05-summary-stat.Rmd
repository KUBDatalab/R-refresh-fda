---
title: "Summary statistics"
teaching: 10
exercises: 5
questions:
- "FIX ME"
objectives:
- "FIX ME"
keypoints:
- "FIX ME"

source: Rmd
math: yes
---

```{r, include = FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
```

## Summary statistics

I vil ofte skulle finde gennemsnit, medianer, standardafvigelser og
andet for jeres data. Det kalder vi for "Summary statistics" fordi 
vi opsummerer en masse til til en enkelt værdi der (forhåbentlig) karakteriserer
den samlede mængde data. 

Vi har også ofte brug for at beregne eksempelvis gennemsnit af en bestemt variabel
for forskellige grupper. 
Et af de
første eksempler I kommer til at arbejde med er smagsdommeres 
vurderinger af smagen af kaffe. Og en ting man kunne have lyst til at 
finde ud af om de forskellige smagsdommere vurderer bitterheden af 
kaffen forskelligt.

Lad os indlæse data, og få vist de første værdier. `head()`-funktionen giver
os de første seks rækker i datasættet:
```{r}
kaffe <- read_excel("../data/Results Panel.xlsx")
head(kaffe)
```

Man kan få et hurtigt overblik ved hjælp af funktionen `summary`:
```{r}
summary(kaffe)
```
Her får vi minimum og maximum af værdierne i de enkelte kolonner, medianen, 
middelværdien og første samt tredie kvartil.

En lidt irriterende ting ved dette datasæt er at kolonnen `Sample`
indeholder temperaturen. Men angivet ikke som `31` og så underforstået
i grader Celsius, men i stedet som `31C`. Det er selvfølgelig en 
fiks måde at få angivet at vi arbejder med forskellige samples, der
er karakteriseret ved at være ved forskellige temperaturer.
Men det betyder at vi ikke kan lave matematik på temperaturerne, og det har vi 
undertiden lyst til.

> ## Hvordan gør vi så det? 
> 
> Under DATAMANIPULATION så vi hvordan vi kunne lave en ny kolonne, baseret på 
> eksisterende kolonner. Det kan vi udnytte her. Vi vil godt have "beregnet" en 
> ny kolonne der skal hedde "Temperatur", baseret på værdierne i "Sample" kolonnen.
> Sample kolonnen indeholder eksempelvis "31C", og vi vil godt have pillet C'et af,
> så vi har 31 tilbage. Og derefter have konverteret 31 til et tal. Det kan vi gøre
> på denne måde:
> 
> kaffe %>% 
>   mutate(Temperatur = str_remove(Sample, "C")) %>% 
>   mutate(Temperatur = as.numeric(Temperatur))
> 
>Her laver vi en ny kolonne "Temperatur" der indeholder resultatet af at køre 
>`str_remove` funktionen på "Sample". Det vi "remover" er "C". Nu står der "31" i 
> Temperatur kolonnen i stedet for "31C". Men "31" er stadig tekst. Og vi kører derfor
> funktionen "as.numeric" på indholdet af Temperatur kolonnen. Den funktion forvander
> teksten "31" til tallet 31.
{: .callout}


Nu kunne vi så godt tænke os at finde ud af om de otte smagsdommere
vurderer eksempelvis bitterhed forskelligt. Er der nogen af dem
der er mere "følsomme" overfor bitterhed end andre. Det kan vi få en
ide om ved at beregne gennemsnittet af deres vurdering af bitterheden
i kaffen for hver af dommerne.
Måske er vi også interesserede i medianen. Eller standardafvigelsen.

Det kan gøres på flere måder. Vi præsenterer her to. Og vi tager udgangspunkt
i spørgsmålet "Hvad er hver enkelt smagsdommers gennemsnitlige vurdering af 
bitterheden i kaffeprøverne?"

## Aggregate muligheden


Dette er måden at besvare spørgsmålet på, som I præsenteres for i 
undervisningsmaterialet.
Den er lidt bøvlet, og kan være vanskelig at vride hjernen omkring.

Den er på den anden side hurtig og nyttig. Og så er det som nævnt den
der er i undervisningsmaterialet, så det er godt at forstå hvad 
der sker.

Funktion vi bruger hedder `aggregate`. Den er indbygget direkte i R.

```{r aggregate-eksempel, eval =F}
aggregate(kaffe, by=list(kaffe$Assessor), FUN = "mean")
```

```{r aggregate-eksempel-med-output,message=F, warning=F, eval =T, echo =F }
aggregate(kaffe, by=list(kaffe$Assessor), FUN = "mean")
```

Hvad sker der her? Aggregatefunktionen tager input, data, i dette 
tilfælde dataframen `kaffe`, og splitter den op i et antal grupper.
Grupperne er defineret af hvad vi skriver i `by` argumentet, og vi 
får her en gruppe. "Group.1" er Assessor.

I gruppe 1 har vi altså dommerne. De har et nummer fra 1 til 8.
Dernæst beregner `aggregate` gennemsnittet af værdierne i *alle* kolonnerne. Det giver os nogle "warnings", fordi vi ikke kan beregne
et gennemsnit af "31C" og "32C". "31C" er tekst og ikke tal. Og vi kan ikke 
beregne gennemsnit af tekst.

Hvordan skal vi læse dem? Vi skal læse dem som at den gennemsnitlige Bitterhed
som dommer 1 har givet, er 9.63125.

Vil vi beregne andet end gennemsnit, erstatter vi "mean" i `FUN` argumentet med 
en anden statistisk funktion, det kunne være "median" eller "sd" for medianen 
hhv. standardafvigelsen.

Hvis vi godt vil dele datasættet op i flere grupper, eksempelvis både efter
smagsdommer og "Sample", kan vi tilføje det i "by=list" delen af funktionen:

```{r eval = F}
aggregate(kaffe, by=list(kaffe$Assessor, kaffe$Sample), FUN = "mean") 
```
```{r echo = F, warning = F}
aggregate(kaffe, by=list(kaffe$Assessor, kaffe$Sample), FUN = "mean") %>% 
  head()
```
Vi får nu både en "Group.1" der er dommernes nummer, og "Group.2" der er "Sample", 
altså temperaturen. Vi får 48 rækker i alt, fordi vi har 8 dommer og seks
forskellige "Sample" værdier. Og dermed gennemsnitlige værdier for hver af de
kombinationer. Her har vi dog begrænset antallet af rækker der vises til seks
af hensyn til overskueligheden.

Læg mærke til at hvis du selv kører denne kode, får du en række "warnings".
Det skyldes at der er tekst i datasættet, og aggregate vil forsøge at beregne
gennemsnittet, også af tekst. Det kan man ikke, og det giver os advarsler.

## Den anden måde.

Vi er glade for at præsentere forskellige måder at nå samme resultat. De der 
finder aggregate metoden vanskelig, finder forhåbentlig denne let.

I tidyverse universet kan vi opnå de samme aggregerede statiske resultater, blot
på en anden måde:

```{r}
kaffe %>% 
  group_by(Assessor) %>% 
  summarise_all(mean)
```

Hvad sker der? Vi tager vores data, `kaffe` og bruger pipen til at sende 
data til funktionen `group_by`. Den "grupperer" datasættet efter værdierne i "Assessor".
Dette grupperede datasæt sendes videre til `summarise_all` funktionen, der beregner
middelværdien - den specificerer vi med "mean", for *alle* kolonnerne.

Der er flere varianter af den. Hvis vi vil fokusere på vurderingen af "Intensity",
kan vi beregne flere statistiske værdier for den på en gang:

```{r}
kaffe %>% 
  group_by(Assessor) %>% 
  summarise(middelværdi = mean(Intensity),
            medianen  = median(Intensity),
            std_afv = sd(Intensity)
            )
  
```
Her bruger vi `summarise` i stedet for `summarise_all`, fordi det ikke er alle
kolonner vi vil lave beregninger på, men kun dem vi specificerer.

Og hvis vi vil opdele vores data efter mere end en parameter, Assessor og Sample
for eksempel, kan vi tilføje de ekstra grupperings variable i `group_by`:

```{r}
kaffe %>% 
  group_by(Assessor, Sample) %>% 
  summarise(middelværdi = mean(Intensity),
            medianen  = median(Intensity),
            std_afv = sd(Intensity)
            )
  
```




## En lineær regression

To variable varierer sammen. Plotter vi det kunne det se således
ud:
```{r}
kaffe %>% 
  ggplot(aes(Bitter, Sour)) +
  geom_point()
```

Indrømmet måske ikke den mest overbevisende lineære
sammenhæng mellem vurderingen af "Bitter" og "Sour".
Men der er noget.

I en lineær regression, forsøger vi, i dette tilfælde,
at forklare variationen i vurderingen af "Sour", ved
hjælp af variationen i "Bitter". Hvis "Bitter" stiger
med "1", hvor meget stiger "Sour" så med.

Med andre ord, vi vil finde den bedste rette linie at
lægge ind i plottet. Sådan en ret linie beskriver vi
som regel med udtrykket $$y = ax + b$$. Eller:

$$Sour = a*Bitter + b$$

Vil vi have R til at lave beregningerne for os, fortæller vi R hvilken lineære model vi vil fitte data
til. Og hvilke data vi arbejder med:

```{r}
model <- lm(Sour ~ Bitter, data = kaffe)
```

Den første del af funktionen, `Sour ~ Bitter` specificerer
at vi vil beskrive værdierne af `Sour` som funktion af
`Bitter`, i datasættet `kaffe`.

Ser vi på outputtet af det - vi gemte resultatet i
objektet `model` får vi følgende:
```{r}
model
```
Hvilket vi kan læse som:
$$Sour = 0.4329*Bitter + 3.8600$$. Og altså som at
hvis en smagsdommer vurderer bitterheden af kaffen til 
at være 13.2, så forudsiger vores model at smagsdommeren
vil vurdere surheden til at være: $$Sour = 0.4329*13.2 + 3.8600 = 9.57428$$.




Vi kan se flere detaljer:
```{r}
summary(model)
```
Her ser vi koefficienterne fra den rette linies ligning, og får også
p-værdierne for dem. Forskellige statistiske tests kalder p-værdien forskellige ting. Her bruges "Pr".

Bemærk at de meget fine p-værdier ikke i sig selv fortæller at der er en sammenhæng, blot at vi med stor sikkerhed kan afvise at koefficienterne skulle være 0.

Bemærk også at $$R^2$$ kun er 0.176, hvilket kan tolkes som at denne lineære model
forklarer 17.6% af variationen i `Sour`. Fantastiske p-værdier, elendig model!

Det allerførste vi ser ovenfor, er statistik på `residualerne`. Det kan vi forstå
som den del af variationen af `Sour`, der ikke forklares af den lineære model. I eksemplet ovenfor, hvor en smagsdommer vurderede
"bitter" til 13.2 forudså modellen at samme smagsdommer skulle vurdere "Sour" til 9.57428. Kigger vi i data, kan vi finde en 
smagsdommer der faktisk vurderer "Bitter" til 13.2. Men samme smagsdommers vurdering af "Sour" er faktisk 11.6. 11.6 - 9.57428 er forskellen på de faktiske data, og det vores lineære model forudsiger. Eller "residualen", den del af variationen i "Sour" som modellen *ikke* forklarer.

{% include links.md %}